# üßπ Kiva Crowdfunding Loans ‚Äì Data Cleaning & Preparation with Python

Este proyecto se centra en el **procesamiento, limpieza y preparaci√≥n de datos** utilizando **Python** en un entorno de **notebook (Jupyter / Google Colab)**, aplicando **buenas pr√°cticas de Data Cleaning** sobre un dataset real de **Kiva Crowdfunding**.

El objetivo principal es transformar datos brutos (*raw data*) en un **dataset estructurado, consistente y reproducible**, listo para su posterior an√°lisis o modelado predictivo, documentando de forma clara **cada decisi√≥n t√©cnica tomada durante el proceso**.

---

## üéØ Objetivos del proyecto

- Comprender la estructura y calidad inicial del dataset.
- Detectar y gestionar valores nulos, duplicados y tipos incorrectos.
- Normalizar y transformar variables clave.
- Crear columnas derivadas con valor anal√≠tico.
- Analizar y documentar outliers.
- Validar la calidad del dataset final.
- Exportar un dataset limpio en formato **CSV**.
- Entregar un notebook reproducible y bien documentado.

---

## üìä Dataset utilizado

- **Nombre:** Data Science for Good ‚Äì Kiva Crowdfunding  
- **Fuente:** Kaggle  
- **Link:** https://www.kaggle.com/datasets/kiva/data-science-for-good-kiva-crowdfunding  
- **Organizaci√≥n:** Kiva.org  

Dataset que contiene informaci√≥n sobre **pr√©stamos de microfinanciaci√≥n** otorgados a nivel global, incluyendo montos, pa√≠ses, sectores, prestamistas, fechas y estado del desembolso.

---

## üß∞ Tecnolog√≠as utilizadas

- **Python 3**
- **Google Colab / Jupyter Notebook**
- **Librer√≠as**
  - pandas
  - numpy
  - matplotlib
  - seaborn
- **Control de versiones:** Git & GitHub
- **Formato de exportaci√≥n:** CSV

---

## üß≠ Estructura del Notebook

1. Importaci√≥n del dataset (local y Google Drive)
2. Exploraci√≥n inicial
   - Dimensiones (shape)
   - Tipos de datos
   - Resumen estad√≠stico
3. Diagn√≥stico de calidad de datos
   - Valores nulos
   - Duplicados
   - Formatos incorrectos
4. Limpieza y transformaciones
5. Creaci√≥n de variables derivadas
6. An√°lisis visual exploratorio
7. Estudio de outliers
8. Validaci√≥n post-limpieza
9. Exportaci√≥n del dataset limpio

---

## üîç Proceso de limpieza y transformaci√≥n

### 1Ô∏è‚É£ Eliminaci√≥n de columnas con alto porcentaje de nulos

Durante la exploraci√≥n inicial se identificaron columnas con muchos valores faltantes y bajo valor anal√≠tico para el estudio:

- tags  
- region  
- partner_id  

Estas columnas fueron eliminadas trabajando siempre sobre una **copia del dataset original**, preservando el raw data.

**Resultado:** dataset m√°s limpio, legible y f√°cil de mantener.

---

### 2Ô∏è‚É£ Correcci√≥n de tipos de datos

Se detect√≥ que varias columnas importantes, incluyendo fechas y valores num√©ricos, estaban almacenadas como tipo `object`.

Acciones realizadas:
- Conversi√≥n de columnas de fecha a tipo `datetime`
- Conversi√≥n de columnas num√©ricas a `int` o `float`

Este paso es fundamental para asegurar an√°lisis, comparaciones y visualizaciones correctas.

---

### 3Ô∏è‚É£ Creaci√≥n de columnas derivadas

#### üîπ Tipo de pr√©stamo seg√∫n fecha de desembolso

Se cre√≥ la columna `loan_type` para clasificar los pr√©stamos seg√∫n el momento del desembolso:

python

kiva_loans_df["loan_type"] = np.where(
    kiva_loans_df["disbursed_time"] < kiva_loans_df["posted_time"],
    "pre_disbursed",
    "post_disbursed"
)
Esto permite analizar diferencias entre pr√©stamos **pre_disbursed** y **post_disbursed**, aportando contexto temporal al estado del pr√©stamo.

---

### üîπ Categorizaci√≥n del monto del pr√©stamo

Se cre√≥ la columna `loan_amount_category`, segmentando el monto del pr√©stamo en:

- **Micro**
- **Small**
- **Medium**
- **Large**

Esta categorizaci√≥n facilita el an√°lisis de distribuciones, comparaciones y patrones por tama√±o de pr√©stamo.

---

### 4Ô∏è‚É£ An√°lisis visual exploratorio

Se utilizaron distintos tipos de visualizaciones seg√∫n el objetivo anal√≠tico:

#### üìä Histogramas
- Mayor concentraci√≥n de pr√©stamos con monto inferior a **2.500**

#### üìä Gr√°ficos de barras
- Top pa√≠ses por `funded_amount` (destaca **Filipinas**)
- Distribuci√≥n por sector (**Agriculture**, **Food**, **Retail**)
- Comparaci√≥n entre pr√©stamos **pre_disbursed** y **post_disbursed**

#### üìà Gr√°ficos de l√≠neas
- Evoluci√≥n mensual del monto medio del pr√©stamo
- Picos claros en **enero**
- Descenso marcado en **2017**, posible l√≠nea de investigaci√≥n futura

#### ü•ß Gr√°ficos circulares
- Proporci√≥n de pr√©stamos por pa√≠s (**Top 8**)

---

### 5Ô∏è‚É£ Visualizaci√≥n geogr√°fica

Se implement√≥ un **choropleth map** para representar visualmente:

- Pa√≠ses
- Cantidad total de pr√©stamos recibidos

Este enfoque facilita la comprensi√≥n del impacto geogr√°fico y social de **Kiva**.

---

### 6Ô∏è‚É£ An√°lisis de outliers

Se realiz√≥ un an√°lisis de valores at√≠picos utilizando:

- Cuartiles (**Q1**, **Q3**)
- Rango intercuart√≠lico (**IQR**)
- Gr√°ficos de densidad

#### üîé Resultados observados

- **Micro:** mayor densidad alrededor de **250**
- **Small:** concentraci√≥n entre **500 y 1.000**
- **Medium:** densidad entre **2.000 y 3.000**, decreciendo hasta **6.000**
- **Large:** pico alrededor de **10.000**, con valores extremos cercanos a **50.000**

Las decisiones de conservaci√≥n o exclusi√≥n de outliers fueron **documentadas y justificadas**.

---

### ‚úÖ Validaci√≥n post-limpieza

Se realizaron comprobaciones finales para asegurar la calidad del dataset:

- Conteos esperados
- Ausencia de nulos en columnas clave
- Coherencia de tipos de datos
- Revisi√≥n de muestras aleatorias

---

### üì§ Exportaci√≥n del dataset limpio

El dataset final fue exportado:

- En formato **CSV**
- Tanto en **Google Drive** como en entorno local

Quedando listo para an√°lisis posterior o modelado predictivo.

---

### üß† Conclusiones

- La limpieza de datos es un proceso cr√≠tico y estructurado
- Documentar cada decisi√≥n mejora la reproducibilidad
- La visualizaci√≥n ayuda a validar transformaciones
- El dataset final es consistente, legible y reutilizable
- Se aplicaron buenas pr√°cticas de **Data Wrangling profesional**

---

### üöÄ Pr√≥ximos pasos

- Exportar una versi√≥n en formato **Parquet**
- Automatizar el pipeline de limpieza
- Feature engineering avanzado
- Modelos predictivos sobre probabilidad de financiaci√≥n

---

### üìå Nota final

Este proyecto refleja un flujo de trabajo realista de un **Data Analyst**, poniendo √©nfasis en:

- Calidad de datos
- Reproducibilidad
- Trazabilidad
- Buen criterio t√©cnico y anal√≠tico
